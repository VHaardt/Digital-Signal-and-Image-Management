{"cells":[{"cell_type":"markdown","source":["# **StarGAN**<br/>\n","**Master's Degree in Data Science (A.Y. 2023/2024)**<br/>\n","**University of Milano - Bicocca**<br/>\n","\n","Vittorio Haardt, Luca Porcelli"],"metadata":{"id":"wYT-yYJ4Ue5x"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17438,"status":"ok","timestamp":1707822823296,"user":{"displayName":"Vittorio Haardt","userId":"02994810218058108833"},"user_tz":-60},"id":"jPjK-Mo-CwZS","outputId":"9956b192-50c9-499f-912d-c89f8b45e712"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","import shutil\n","import random\n","from PIL import Image\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import tensorflow as tf\n","from torch.utils import data\n","from torchvision import transforms as T\n","from torchvision.datasets import ImageFolder\n","from torch.autograd import Variable\n","from torchvision.utils import save_image\n","import time\n","import datetime\n","import argparse\n","from torch.backends import cudnn"],"metadata":{"id":"VKW5orCJVZyE","executionInfo":{"status":"ok","timestamp":1707822830962,"user_tz":-60,"elapsed":7670,"user":{"displayName":"Vittorio Haardt","userId":"02994810218058108833"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Train Data"],"metadata":{"id":"uahiANkBtS3-"}},{"cell_type":"markdown","source":["Import the training data from the zipped file"],"metadata":{"id":"uw7OralzU0hS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DpoMmio3C1lg"},"outputs":[],"source":["!unzip \"/content/drive/MyDrive/Digital/Data/Imm.zip\" -d Data"]},{"cell_type":"markdown","source":["Remove the unwanted class"],"metadata":{"id":"pMEbkmc3Vd5A"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SKjPUOIAC6VL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707755165235,"user_tz":-60,"elapsed":41,"user":{"displayName":"GAN","userId":"09141957892611562282"}},"outputId":"e5ae95a0-92ea-44db-a1ca-0c95b44c6ff9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Eliminato il file: /content/Data/dataset/Sad/0b1040152218828705966fe7b6ae4d84ef194c3fec5a8853b64aa14f.bmp\n","Eliminato il file: /content/Data/dataset/Happy/2b36e0f43db9afb8112bbebd145789a4f6c35a7098eec295f0237d20.bmp\n","Eliminato il file: /content/Data/dataset/Angry/bb807308129b1810a9789477d7587af2838cb6ebb6e6e724d59ae34b~angry.bmp\n","Eliminato il file: /content/Data/dataset/Neutral/0a20898ed0ae9f058112516f3bff1885f57e71188b148300caf23e37f.bmp\n","Eliminato il file: /content/Data/dataset/Neutral/0c44e659d8adc399e063533722f72fd8fe885905f1aa5f0e07928f64f.bmp\n"]}],"source":["cartella_principale = '/content/Data/dataset'\n","\n","cartella_da_elimare = os.path.join(cartella_principale, 'Ahegao')\n","shutil.rmtree(cartella_da_elimare)\n","\n","# List of allowed extension\n","estensioni_consentite = ['.jpg', '.jpeg', '.png']\n","\n","# Controll if a file has a proibited extension\n","def is_estensione_consentita(file):\n","    return any(file.lower().endswith(ext) for ext in estensioni_consentite)\n","\n","# Itera through the main folder and subfolders\n","for cartella_padre, _, files in os.walk(cartella_principale):\n","    for file in files:\n","        percorso_file = os.path.join(cartella_padre, file)\n","        if not is_estensione_consentita(file):\n","            os.remove(percorso_file)\n","            print(f\"Eliminato il file: {percorso_file}\")"]},{"cell_type":"markdown","source":["We resize all images to the same dimension and in the format of 256x256 so that during the generation process, no important parts of the faces are cut off."],"metadata":{"id":"jo_-7lhkXPaS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_jthK6dO6dzN"},"outputs":[],"source":["def make_square(image_path, output_size=(256, 256)):\n","    \"\"\"\n","    The function renders a square image by centering and resizing it.\n","    :param image_path: The path to the original image.\n","    :param output_size: The desired size of the output square image.\n","    :return: The square image.\n","    \"\"\"\n","    img = Image.open(image_path)\n","    img = img.resize(output_size, Image.ANTIALIAS)\n","    new_img = Image.new(\"RGB\", output_size, (255, 255, 255))\n","    position = ((output_size[0] - img.size[0]) // 2, (output_size[1] - img.size[1]) // 2)\n","    new_img.paste(img, position)\n","    return new_img\n","\n","def process_images_in_directory(input_dir, output_dir):\n","    \"\"\"\n","    Processes all the images present in a folder and makes them square.\n","    :param input_dir: Input folder containing the images.\n","    :param output_dir: Output folder where the square images will be saved.\n","    \"\"\"\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","    for root, dirs, files in os.walk(input_dir):\n","        for file in tqdm(files):\n","            if file.endswith(\".jpg\") or file.endswith(\".jpeg\") or file.endswith(\".png\"):\n","                input_path = os.path.join(root, file)\n","                output_subdir = os.path.join(output_dir, os.path.relpath(root, input_dir))\n","                output_path = os.path.join(output_subdir, file)\n","                if not os.path.exists(output_subdir):\n","                    os.makedirs(output_subdir)\n","                square_img = make_square(input_path)\n","                square_img.save(output_path)\n"]},{"cell_type":"code","source":["input_directory = \"/content/Data/dataset\"\n","output_directory = \"/content/Data/dataset_quad\"\n","process_images_in_directory(input_directory, output_directory)"],"metadata":{"id":"SllmxnLSuOEC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707755485443,"user_tz":-60,"elapsed":313556,"user":{"displayName":"GAN","userId":"09141957892611562282"}},"outputId":"0daaabb2-457b-43b6-8afa-3456e65338f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["0it [00:00, ?it/s]\n","  0%|          | 0/3933 [00:00<?, ?it/s]<ipython-input-5-2eab3d3d7807>:13: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n","  img = img.resize(output_size, Image.ANTIALIAS)\n","100%|██████████| 3933/3933 [01:17<00:00, 50.94it/s]\n","100%|██████████| 3739/3739 [01:08<00:00, 54.39it/s]\n","100%|██████████| 1312/1312 [00:37<00:00, 35.09it/s]\n","100%|██████████| 1234/1234 [00:46<00:00, 26.46it/s]\n","100%|██████████| 4025/4025 [01:23<00:00, 48.23it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"PZIjmDdlDC1k"},"source":["## STAR GAN Functions"]},{"cell_type":"markdown","source":["Residual block used in the models.\n","\n","*   Discriminator\n","*   Generator\n","\n","Discriminator and generator models that are used in the GAN architecture."],"metadata":{"id":"RuUQlVVrYNCS"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"2lv2a2v8C9wU","executionInfo":{"status":"ok","timestamp":1707822858470,"user_tz":-60,"elapsed":249,"user":{"displayName":"Vittorio Haardt","userId":"02994810218058108833"}}},"outputs":[],"source":["class ResidualBlock(nn.Module):\n","    \"\"\"Residual Block with instance normalization.\"\"\"\n","    def __init__(self, dim_in, dim_out):\n","        super(ResidualBlock, self).__init__()\n","        self.main = nn.Sequential(\n","            nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True))\n","\n","    def forward(self, x):\n","        return x + self.main(x)\n","\n","\n","class Generator(nn.Module):\n","    \"\"\"Generator network.\"\"\"\n","    def __init__(self, conv_dim=64, c_dim=5, repeat_num=6):\n","        super(Generator, self).__init__()\n","\n","        layers = []\n","        layers.append(nn.Conv2d(3+c_dim, conv_dim, kernel_size=7, stride=1, padding=3, bias=False))\n","        layers.append(nn.InstanceNorm2d(conv_dim, affine=True, track_running_stats=True))\n","        layers.append(nn.ReLU(inplace=True))\n","\n","        # Down-sampling layers.\n","        curr_dim = conv_dim\n","        for i in range(2):\n","            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1, bias=False))\n","            layers.append(nn.InstanceNorm2d(curr_dim*2, affine=True, track_running_stats=True))\n","            layers.append(nn.ReLU(inplace=True))\n","            curr_dim = curr_dim * 2\n","\n","        # Bottleneck layers.\n","        for i in range(repeat_num):\n","            layers.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim))\n","\n","        # Up-sampling layers.\n","        for i in range(2):\n","            layers.append(nn.ConvTranspose2d(curr_dim, curr_dim//2, kernel_size=4, stride=2, padding=1, bias=False))\n","            layers.append(nn.InstanceNorm2d(curr_dim//2, affine=True, track_running_stats=True))\n","            layers.append(nn.ReLU(inplace=True))\n","            curr_dim = curr_dim // 2\n","\n","        layers.append(nn.Conv2d(curr_dim, 3, kernel_size=7, stride=1, padding=3, bias=False))\n","        layers.append(nn.Tanh())\n","        self.main = nn.Sequential(*layers)\n","\n","    def forward(self, x, c):\n","        c = c.view(c.size(0), c.size(1), 1, 1)\n","        c = c.repeat(1, 1, x.size(2), x.size(3))\n","        x = torch.cat([x, c], dim=1)\n","        return self.main(x)\n","\n","\n","class Discriminator(nn.Module):\n","    \"\"\"Discriminator network with PatchGAN.\"\"\"\n","    def __init__(self, image_size=128, conv_dim=64, c_dim=5, repeat_num=6):\n","        super(Discriminator, self).__init__()\n","        layers = []\n","        layers.append(nn.Conv2d(3, conv_dim, kernel_size=4, stride=2, padding=1))\n","        layers.append(nn.LeakyReLU(0.01))\n","\n","        curr_dim = conv_dim\n","        for i in range(1, repeat_num):\n","            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1))\n","            layers.append(nn.LeakyReLU(0.01))\n","            curr_dim = curr_dim * 2\n","\n","        kernel_size = int(image_size / np.power(2, repeat_num))\n","        self.main = nn.Sequential(*layers)\n","        self.conv1 = nn.Conv2d(curr_dim, 1, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.conv2 = nn.Conv2d(curr_dim, c_dim, kernel_size=kernel_size, bias=False)\n","\n","    def forward(self, x):\n","        h = self.main(x)\n","        out_src = self.conv1(h)\n","        out_cls = self.conv2(h)\n","        return out_src, out_cls.view(out_cls.size(0), out_cls.size(1))"]},{"cell_type":"markdown","source":["Wrapper for logging data to TensorBoard, which is a visualization tool used for monitoring and understanding the behavior of TensorFlow models."],"metadata":{"id":"e4vnU6qaZGQ2"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"yqK2Bwy0DJLV","executionInfo":{"status":"ok","timestamp":1707822869298,"user_tz":-60,"elapsed":2,"user":{"displayName":"Vittorio Haardt","userId":"02994810218058108833"}}},"outputs":[],"source":["class Logger(object):\n","    \"\"\"Tensorboard logger.\"\"\"\n","\n","    def __init__(self, log_dir):\n","        \"\"\"Initialize summary writer.\"\"\"\n","        self.writer = tf.summary.create_file_writer(log_dir)\n","\n","    def scalar_summary(self, tag, value, step):\n","        \"\"\"Add scalar summary.\"\"\"\n","        with self.writer.as_default():\n","            tf.summary.scalar(tag, value, step=step)\n","            self.writer.flush()"]},{"cell_type":"markdown","source":["Data loader: The loader apply some trasformation to the imput image, and in the training some data agumentation."],"metadata":{"id":"wpLLhAMiZXhA"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"bnXo2QYPDLa6","executionInfo":{"status":"ok","timestamp":1707822870672,"user_tz":-60,"elapsed":249,"user":{"displayName":"Vittorio Haardt","userId":"02994810218058108833"}}},"outputs":[],"source":["def get_loader(image_dir, attr_path, selected_attrs, crop_size=178, image_size=224,\n","               batch_size=16, mode='train', num_workers=1):\n","    \"\"\"Build and return a data loader.\"\"\"\n","    transform = []\n","    if mode == 'train':\n","        transform.append(T.RandomHorizontalFlip())\n","    transform.append(T.CenterCrop(crop_size))\n","    transform.append(T.Resize(image_size))\n","    transform.append(T.ToTensor())\n","    transform.append(T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)))\n","    transform = T.Compose(transform)\n","\n","    dataset = ImageFolder(image_dir, transform)\n","\n","    data_loader = data.DataLoader(dataset=dataset,\n","                                  batch_size=batch_size,\n","                                  shuffle=(mode=='train'),\n","                                  num_workers=num_workers)\n","    return data_loader"]},{"cell_type":"markdown","source":["Model solver for training the StarGAN, the training parmaeter are the following:\n","* dimension of domain labels: 5\n","* image resolution: 224\n","* number of conv filters in the first layer of G: 64\n","* number of conv filters in the first layer of D: 64\n","* number of residual blocks in G: 6\n","* number of strided conv layers in D: 6\n","* weight for domain classification loss: 1\n","* weight for reconstruction loss: 10\n","* weight for gradient penalty: 10\n","* mini-batch size: 16\n","* number of total iterations for training D: 200000\n","* number of iterations for decaying lr: 100000\n","* learning rate for G: g_lr\n","* learning rate for D: d_lr\n","* number of D updates per each G update: 5\n","* beta1 for Adam optimizer: 0.5\n","* beta2 for Adam optimizer: 0.999"],"metadata":{"id":"es3CmcoPZ15m"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"74qT0F3SDT7n","executionInfo":{"status":"ok","timestamp":1707822878446,"user_tz":-60,"elapsed":242,"user":{"displayName":"Vittorio Haardt","userId":"02994810218058108833"}}},"outputs":[],"source":["class Solver(object):\n","    \"\"\"Solver for training StarGAN.\"\"\"\n","\n","    def __init__(self, rafd_loader = None, resume_iters = None, g_lr = 0.0001, d_lr = 0.0001):\n","        \"\"\"Initialize configurations.\"\"\"\n","\n","        # Data loader.\n","        self.rafd_loader = rafd_loader\n","\n","        # Model configurations.\n","        self.c_dim = 5\n","        self.image_size = 224\n","        self.g_conv_dim = 64\n","        self.d_conv_dim = 64\n","        self.g_repeat_num = 6\n","        self.d_repeat_num = 6\n","        self.lambda_cls = 1\n","        self.lambda_rec = 10\n","        self.lambda_gp = 10\n","\n","        # Training configurations.\n","        self.batch_size = 16\n","        self.num_iters = 200000 #\n","        self.num_iters_decay = 100000\n","        self.g_lr = g_lr\n","        self.d_lr = d_lr\n","        self.n_critic = 5\n","        self.beta1 = 0.5\n","        self.beta2 = 0.999\n","        self.resume_iters = resume_iters\n","        self.selected_attrs = ['Angry', 'Happy', 'Neutral', 'Sad', 'Surprise']\n","\n","        # Miscellaneous.\n","        self.use_tensorboard = 'True'\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","        # Directories.\n","        self.log_dir = '/content/drive/MyDrive/Digital/stargan/logs'\n","        self.sample_dir = '/content/drive/MyDrive/Digital/stargan/samples'\n","        self.model_save_dir = '/content/drive/MyDrive/Digital/stargan/models'\n","        self.result_dir = '/content/drive/MyDrive/Digital/stargan/results'\n","\n","        # Step size.\n","        self.log_step = 10\n","        self.sample_step = 1000 #\n","        self.model_save_step = 2500 #\n","        self.lr_update_step = 1000\n","\n","        # Build the model and tensorboard.\n","        self.build_model()\n","        if self.use_tensorboard:\n","            self.build_tensorboard()\n","\n","    def build_model(self):\n","        \"\"\"Create a generator and a discriminator.\"\"\"\n","        self.G = Generator(self.g_conv_dim, self.c_dim, self.g_repeat_num)\n","        self.D = Discriminator(self.image_size, self.d_conv_dim, self.c_dim, self.d_repeat_num)\n","\n","        self.g_optimizer = torch.optim.Adam(self.G.parameters(), self.g_lr, [self.beta1, self.beta2])\n","        self.d_optimizer = torch.optim.Adam(self.D.parameters(), self.d_lr, [self.beta1, self.beta2])\n","        self.print_network(self.G, 'G')\n","        self.print_network(self.D, 'D')\n","\n","        self.G.to(self.device)\n","        self.D.to(self.device)\n","\n","    def print_network(self, model, name):\n","        \"\"\"Print out the network information.\"\"\"\n","        num_params = 0\n","        for p in model.parameters():\n","            num_params += p.numel()\n","        print(model)\n","        print(name)\n","        print(\"The number of parameters: {}\".format(num_params))\n","\n","    def restore_model(self, resume_iters):\n","        \"\"\"Restore the trained generator and discriminator.\"\"\"\n","        print('Loading the trained models from step {}...'.format(resume_iters))\n","        G_path = os.path.join(self.model_save_dir, '{}-G.ckpt'.format(resume_iters))\n","        D_path = os.path.join(self.model_save_dir, '{}-D.ckpt'.format(resume_iters))\n","        self.G.load_state_dict(torch.load(G_path, map_location=lambda storage, loc: storage))\n","        self.D.load_state_dict(torch.load(D_path, map_location=lambda storage, loc: storage))\n","\n","    def build_tensorboard(self):\n","        \"\"\"Build a tensorboard logger.\"\"\"\n","        self.logger = Logger(self.log_dir)\n","\n","    def update_lr(self, g_lr, d_lr):\n","        \"\"\"Decay learning rates of the generator and discriminator.\"\"\"\n","        for param_group in self.g_optimizer.param_groups:\n","            param_group['lr'] = g_lr\n","        for param_group in self.d_optimizer.param_groups:\n","            param_group['lr'] = d_lr\n","\n","    def reset_grad(self):\n","        \"\"\"Reset the gradient buffers.\"\"\"\n","        self.g_optimizer.zero_grad()\n","        self.d_optimizer.zero_grad()\n","\n","    def denorm(self, x):\n","        \"\"\"Convert the range from [-1, 1] to [0, 1].\"\"\"\n","        out = (x + 1) / 2\n","        return out.clamp_(0, 1)\n","\n","    def gradient_penalty(self, y, x):\n","        \"\"\"Compute gradient penalty: (L2_norm(dy/dx) - 1)**2.\"\"\"\n","        weight = torch.ones(y.size()).to(self.device)\n","        dydx = torch.autograd.grad(outputs=y,\n","                                   inputs=x,\n","                                   grad_outputs=weight,\n","                                   retain_graph=True,\n","                                   create_graph=True,\n","                                   only_inputs=True)[0]\n","\n","        dydx = dydx.view(dydx.size(0), -1)\n","        dydx_l2norm = torch.sqrt(torch.sum(dydx**2, dim=1))\n","        return torch.mean((dydx_l2norm-1)**2)\n","\n","    def label2onehot(self, labels, dim):\n","        \"\"\"Convert label indices to one-hot vectors.\"\"\"\n","        batch_size = labels.size(0)\n","        out = torch.zeros(batch_size, dim)\n","        out[np.arange(batch_size), labels.long()] = 1\n","        return out\n","\n","    def create_labels(self, c_org, c_dim=5, selected_attrs=None):\n","        \"\"\"Generate target domain labels for debugging and testing.\"\"\"\n","        c_trg_list = []\n","        for i in range(c_dim):\n","            c_trg = self.label2onehot(torch.ones(c_org.size(0))*i, c_dim)\n","\n","            c_trg_list.append(c_trg.to(self.device))\n","        return c_trg_list\n","\n","    def classification_loss(self, logit, target):\n","        \"\"\"Compute binary or softmax cross entropy loss.\"\"\"\n","        return F.cross_entropy(logit, target)\n","\n","    def train(self):\n","        \"\"\"Train StarGAN within a single dataset.\"\"\"\n","        # Set data loader.\n","        data_loader = self.rafd_loader\n","\n","        # Fetch fixed inputs for debugging.\n","        data_iter = iter(data_loader)\n","        x_fixed, c_org = next(data_iter)\n","        x_fixed = x_fixed.to(self.device)\n","        c_fixed_list = self.create_labels(c_org, self.c_dim, self.selected_attrs)\n","\n","        # Learning rate cache for decaying.\n","        g_lr = self.g_lr\n","        d_lr = self.d_lr\n","        #print(g_lr)\n","        #print(d_lr)\n","\n","        # Start training from scratch or resume training.\n","        start_iters = 0\n","        if self.resume_iters:\n","            print('resuming')\n","            start_iters = self.resume_iters\n","            self.restore_model(self.resume_iters)\n","\n","        # Start training.\n","        print('Start training...')\n","        start_time = time.time()\n","        for i in range(start_iters, self.num_iters):\n","\n","            # =================================================================================== #\n","            #                             1. Preprocess input data                                #\n","            # =================================================================================== #\n","\n","            # Fetch real images and labels.\n","            try:\n","                x_real, label_org = next(data_iter)\n","            except:\n","                data_iter = iter(data_loader)\n","                x_real, label_org = next(data_iter)\n","\n","            # Generate target domain labels randomly.\n","            rand_idx = torch.randperm(label_org.size(0))\n","            label_trg = label_org[rand_idx]\n","\n","            c_org = self.label2onehot(label_org, self.c_dim)\n","            c_trg = self.label2onehot(label_trg, self.c_dim)\n","\n","            x_real = x_real.to(self.device)           # Input images.\n","            c_org = c_org.to(self.device)             # Original domain labels.\n","            c_trg = c_trg.to(self.device)             # Target domain labels.\n","            label_org = label_org.to(self.device)     # Labels for computing classification loss.\n","            label_trg = label_trg.to(self.device)     # Labels for computing classification loss.\n","\n","            # =================================================================================== #\n","            #                             2. Train the discriminator                              #\n","            # =================================================================================== #\n","\n","            # Compute loss with real images.\n","            out_src, out_cls = self.D(x_real)\n","            d_loss_real = - torch.mean(out_src)\n","            d_loss_cls = self.classification_loss(out_cls, label_org)\n","\n","            # Compute loss with fake images.\n","            x_fake = self.G(x_real, c_trg)\n","            out_src, out_cls = self.D(x_fake.detach())\n","            d_loss_fake = torch.mean(out_src)\n","\n","            # Compute loss for gradient penalty.\n","            alpha = torch.rand(x_real.size(0), 1, 1, 1).to(self.device)\n","            x_hat = (alpha * x_real.data + (1 - alpha) * x_fake.data).requires_grad_(True)\n","            out_src, _ = self.D(x_hat)\n","            d_loss_gp = self.gradient_penalty(out_src, x_hat)\n","\n","            # Backward and optimize.\n","            d_loss = d_loss_real + d_loss_fake + self.lambda_cls * d_loss_cls + self.lambda_gp * d_loss_gp\n","            self.reset_grad()\n","            d_loss.backward()\n","            self.d_optimizer.step()\n","\n","            # Logging.\n","            loss = {}\n","            loss['D/loss_real'] = d_loss_real.item()\n","            loss['D/loss_fake'] = d_loss_fake.item()\n","            loss['D/loss_cls'] = d_loss_cls.item()\n","            loss['D/loss_gp'] = d_loss_gp.item()\n","\n","            # =================================================================================== #\n","            #                               3. Train the generator                                #\n","            # =================================================================================== #\n","\n","            if (i+1) % self.n_critic == 0:\n","                # Original-to-target domain.\n","                x_fake = self.G(x_real, c_trg)\n","                out_src, out_cls = self.D(x_fake)\n","                g_loss_fake = - torch.mean(out_src)\n","                g_loss_cls = self.classification_loss(out_cls, label_trg)\n","\n","                # Target-to-original domain.\n","                x_reconst = self.G(x_fake, c_org)\n","                g_loss_rec = torch.mean(torch.abs(x_real - x_reconst))\n","\n","                # Backward and optimize.\n","                g_loss = g_loss_fake + self.lambda_rec * g_loss_rec + self.lambda_cls * g_loss_cls\n","                self.reset_grad()\n","                g_loss.backward()\n","                self.g_optimizer.step()\n","\n","                # Logging.\n","                loss['G/loss_fake'] = g_loss_fake.item()\n","                loss['G/loss_rec'] = g_loss_rec.item()\n","                loss['G/loss_cls'] = g_loss_cls.item()\n","\n","            # =================================================================================== #\n","            #                                 4. Miscellaneous                                    #\n","            # =================================================================================== #\n","\n","            # Print out training information.\n","            if (i+1) % self.log_step == 0:\n","                et = time.time() - start_time\n","                et = str(datetime.timedelta(seconds=et))[:-7]\n","                log = \"Elapsed [{}], Iteration [{}/{}]\".format(et, i+1, self.num_iters)\n","                for tag, value in loss.items():\n","                    log += \", {}: {:.4f}\".format(tag, value)\n","                print(log)\n","\n","                if self.use_tensorboard:\n","                    for tag, value in loss.items():\n","                        self.logger.scalar_summary(tag, value, i+1)\n","\n","            # Translate fixed images for debugging.\n","            if (i+1) % self.sample_step == 0:\n","                with torch.no_grad():\n","                    x_fake_list = [x_fixed]\n","                    for c_fixed in c_fixed_list:\n","                        x_fake_list.append(self.G(x_fixed, c_fixed))\n","                    x_concat = torch.cat(x_fake_list, dim=3)\n","                    sample_path = os.path.join(self.sample_dir, '{}-images.jpg'.format(i+1))\n","                    save_image(self.denorm(x_concat.data.cpu()), sample_path, nrow=1, padding=0)\n","                    print('Saved real and fake images into {}...'.format(sample_path))\n","\n","            # Save model checkpoints.\n","            if (i+1) % self.model_save_step == 0:\n","                G_path = os.path.join(self.model_save_dir, '{}-G.ckpt'.format(i+1))\n","                D_path = os.path.join(self.model_save_dir, '{}-D.ckpt'.format(i+1))\n","                torch.save(self.G.state_dict(), G_path)\n","                torch.save(self.D.state_dict(), D_path)\n","                print('Saved model checkpoints into {}...'.format(self.model_save_dir))\n","\n","            # Decay learning rates.\n","            if (i+1) % self.lr_update_step == 0 and (i+1) > (self.num_iters - self.num_iters_decay):\n","                g_lr -= (self.g_lr / float(self.num_iters_decay))\n","                d_lr -= (self.d_lr / float(self.num_iters_decay))\n","                self.update_lr(g_lr, d_lr)\n","                print ('Decayed learning rates, g_lr: {}, d_lr: {}.'.format(g_lr, d_lr))\n","\n"]},{"cell_type":"markdown","source":["## Train"],"metadata":{"id":"uFdEN-8PtqGX"}},{"cell_type":"markdown","source":["Load the trainingg data and the solver"],"metadata":{"id":"ugcefcowdZsO"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2211,"status":"ok","timestamp":1707755496653,"user":{"displayName":"GAN","userId":"09141957892611562282"},"user_tz":-60},"id":"m-XWXzS-FFa6","outputId":"425e511b-697f-49c0-d700-ddbae8f573c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["The number of parameters: 8430528\n","The number of parameters: 44813248\n"]}],"source":["rafd_loader = get_loader('/content/Data/dataset_quad', None, None, 224, 224, 16, 'train', 1)\n","\n","\n","solver = Solver(rafd_loader)"]},{"cell_type":"markdown","source":["Actual model train"],"metadata":{"id":"viXR3xA-djUC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FChvLlA3DdsP"},"outputs":[],"source":["solver.train()"]},{"cell_type":"markdown","metadata":{"id":"VrlExMjM6SrK"},"source":["## Test"]},{"cell_type":"markdown","source":["Extract test data"],"metadata":{"id":"NhhNu4SMdoqj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-5wQC1oE6S7f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707759574640,"user_tz":-60,"elapsed":379,"user":{"displayName":"GAN","userId":"09141957892611562282"}},"outputId":"e4130593-f771-4bfe-ca16-59d00acaa8e8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/drive/MyDrive/Digital/Data/test_prova 2.zip\n","   creating: Test5/test_prova 2/\n","  inflating: Test5/__MACOSX/._test_prova 2  \n","   creating: Test5/test_prova 2/Happy/\n","  inflating: Test5/test_prova 2/.DS_Store  \n","  inflating: Test5/__MACOSX/test_prova 2/._.DS_Store  \n"," extracting: Test5/test_prova 2/Icon  \n","  inflating: Test5/__MACOSX/test_prova 2/._Icon  \n","   creating: Test5/test_prova 2/Sad/\n","   creating: Test5/test_prova 2/Surprise/\n","   creating: Test5/test_prova 2/Neutral/\n","   creating: Test5/test_prova 2/Angry/\n","  inflating: Test5/test_prova 2/Happy/WhatsApp-Image-2024-02-12-at-15.57.40-_1_.png  \n","  inflating: Test5/__MACOSX/test_prova 2/Happy/._WhatsApp-Image-2024-02-12-at-15.57.40-_1_.png  \n","  inflating: Test5/test_prova 2/Happy/.DS_Store  \n","  inflating: Test5/__MACOSX/test_prova 2/Happy/._.DS_Store  \n","  inflating: Test5/test_prova 2/Happy/WhatsApp-Image-2024-02-12-at-15.57.39.png  \n","  inflating: Test5/__MACOSX/test_prova 2/Happy/._WhatsApp-Image-2024-02-12-at-15.57.39.png  \n","  inflating: Test5/test_prova 2/Happy/WhatsApp-Image-2024-02-12-at-15.57.38.png  \n","  inflating: Test5/__MACOSX/test_prova 2/Happy/._WhatsApp-Image-2024-02-12-at-15.57.38.png  \n","  inflating: Test5/test_prova 2/Happy/WhatsApp-Image-2024-02-12-at-15.57.36.png  \n","  inflating: Test5/__MACOSX/test_prova 2/Happy/._WhatsApp-Image-2024-02-12-at-15.57.36.png  \n","  inflating: Test5/test_prova 2/Happy/WhatsApp-Image-2024-02-12-at-15.57.38-_4_.png  \n","  inflating: Test5/__MACOSX/test_prova 2/Happy/._WhatsApp-Image-2024-02-12-at-15.57.38-_4_.png  \n","  inflating: Test5/test_prova 2/Sad/.DS_Store  \n","  inflating: Test5/__MACOSX/test_prova 2/Sad/._.DS_Store  \n","  inflating: Test5/test_prova 2/Sad/WhatsApp Image 2024-02-12 at 15.57.40.jpeg  \n","  inflating: Test5/__MACOSX/test_prova 2/Sad/._WhatsApp Image 2024-02-12 at 15.57.40.jpeg  \n","  inflating: Test5/test_prova 2/Sad/WhatsApp Image 2024-02-12 at 15.57.34.jpeg  \n","  inflating: Test5/__MACOSX/test_prova 2/Sad/._WhatsApp Image 2024-02-12 at 15.57.34.jpeg  \n","  inflating: Test5/test_prova 2/Sad/WhatsApp Image 2024-02-12 at 15.57.39 (1).jpeg  \n","  inflating: Test5/__MACOSX/test_prova 2/Sad/._WhatsApp Image 2024-02-12 at 15.57.39 (1).jpeg  \n","  inflating: Test5/test_prova 2/Surprise/WhatsApp Image 2024-02-12 at 15.57.36 (1).jpeg  \n","  inflating: Test5/__MACOSX/test_prova 2/Surprise/._WhatsApp Image 2024-02-12 at 15.57.36 (1).jpeg  \n","  inflating: Test5/test_prova 2/Surprise/.DS_Store  \n","  inflating: Test5/__MACOSX/test_prova 2/Surprise/._.DS_Store  \n","  inflating: Test5/test_prova 2/Surprise/WhatsApp Image 2024-02-12 at 15.57.38 (2).jpeg  \n","  inflating: Test5/__MACOSX/test_prova 2/Surprise/._WhatsApp Image 2024-02-12 at 15.57.38 (2).jpeg  \n","  inflating: Test5/test_prova 2/Surprise/WhatsApp Image 2024-02-12 at 15.57.35.jpeg  \n","  inflating: Test5/__MACOSX/test_prova 2/Surprise/._WhatsApp Image 2024-02-12 at 15.57.35.jpeg  \n","  inflating: Test5/test_prova 2/Neutral/WhatsApp Image 2024-02-12 at 15.57.41 (1).jpeg  \n","  inflating: Test5/__MACOSX/test_prova 2/Neutral/._WhatsApp Image 2024-02-12 at 15.57.41 (1).jpeg  \n","  inflating: Test5/test_prova 2/Neutral/.DS_Store  \n","  inflating: Test5/__MACOSX/test_prova 2/Neutral/._.DS_Store  \n","  inflating: Test5/test_prova 2/Neutral/WhatsApp Image 2024-02-12 at 15.57.39 (4).jpeg  \n","  inflating: Test5/__MACOSX/test_prova 2/Neutral/._WhatsApp Image 2024-02-12 at 15.57.39 (4).jpeg  \n","  inflating: Test5/test_prova 2/Neutral/WhatsApp Image 2024-02-12 at 18.30.37.jpeg  \n","  inflating: Test5/__MACOSX/test_prova 2/Neutral/._WhatsApp Image 2024-02-12 at 18.30.37.jpeg  \n","  inflating: Test5/test_prova 2/Neutral/WhatsApp Image 2024-02-12 at 15.57.38 (3).jpeg  \n","  inflating: Test5/__MACOSX/test_prova 2/Neutral/._WhatsApp Image 2024-02-12 at 15.57.38 (3).jpeg  \n","  inflating: Test5/test_prova 2/Neutral/WhatsApp Image 2024-02-12 at 15.57.39 (3).jpeg  \n","  inflating: Test5/__MACOSX/test_prova 2/Neutral/._WhatsApp Image 2024-02-12 at 15.57.39 (3).jpeg  \n","  inflating: Test5/test_prova 2/Neutral/WhatsApp Image 2024-02-12 at 15.57.38 (1).jpeg  \n","  inflating: Test5/__MACOSX/test_prova 2/Neutral/._WhatsApp Image 2024-02-12 at 15.57.38 (1).jpeg  \n","  inflating: Test5/test_prova 2/Angry/.DS_Store  \n","  inflating: Test5/__MACOSX/test_prova 2/Angry/._.DS_Store  \n","  inflating: Test5/test_prova 2/Angry/WhatsApp-Image-2024-02-12-at-15.57.35-_2_.jpg  \n","  inflating: Test5/__MACOSX/test_prova 2/Angry/._WhatsApp-Image-2024-02-12-at-15.57.35-_2_.jpg  \n","  inflating: Test5/test_prova 2/Angry/WhatsApp-Image-2024-02-12-at-15.57.35-_1_.jpg  \n","  inflating: Test5/__MACOSX/test_prova 2/Angry/._WhatsApp-Image-2024-02-12-at-15.57.35-_1_.jpg  \n","  inflating: Test5/test_prova 2/Angry/WhatsApp-Image-2024-02-12-at-15.57.39-_2_.jpg  \n","  inflating: Test5/__MACOSX/test_prova 2/Angry/._WhatsApp-Image-2024-02-12-at-15.57.39-_2_.jpg  \n","  inflating: Test5/test_prova 2/Angry/WhatsApp-Image-2024-02-12-at-15.57.40-_2_.jpg  \n","  inflating: Test5/__MACOSX/test_prova 2/Angry/._WhatsApp-Image-2024-02-12-at-15.57.40-_2_.jpg  \n"]}],"source":["!unzip \"/content/drive/MyDrive/Digital/Data/test_prova 2.zip\" -d Test5"]},{"cell_type":"markdown","source":["Resize the test images"],"metadata":{"id":"WxRYdqVVdrJL"}},{"cell_type":"code","source":["input_directory = \"/content/Test5/test_prova 2\"\n","output_directory = \"/content/Test5/test_quad 2\"\n","process_images_in_directory(input_directory, output_directory)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lwz6-vAqt6eV","executionInfo":{"status":"ok","timestamp":1707759587973,"user_tz":-60,"elapsed":1050,"user":{"displayName":"GAN","userId":"09141957892611562282"}},"outputId":"0c1697c6-5c1f-40b1-bcc2-52e77e9effdd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2/2 [00:00<00:00, 2883.67it/s]\n","  0%|          | 0/4 [00:00<?, ?it/s]<ipython-input-5-2eab3d3d7807>:13: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n","  img = img.resize(output_size, Image.ANTIALIAS)\n","100%|██████████| 4/4 [00:00<00:00, 27.75it/s]\n","100%|██████████| 6/6 [00:00<00:00, 12.07it/s]\n","100%|██████████| 5/5 [00:00<00:00, 33.83it/s]\n","100%|██████████| 4/4 [00:00<00:00, 29.15it/s]\n","100%|██████████| 7/7 [00:00<00:00, 40.95it/s]\n"]}]},{"cell_type":"markdown","source":["Load the test data"],"metadata":{"id":"v4EiZc8Fd77y"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8FW1lWD5y8cL"},"outputs":[],"source":["prova_loader = get_loader('/content/Test5/test_quad 2', None, None, 224, 224, 16, 'test', 1)"]},{"cell_type":"markdown","source":["Call the trained solver"],"metadata":{"id":"UKk6iseid909"}},{"cell_type":"code","source":["solver = Solver(prova_loader, resume_iters = 200000)"],"metadata":{"id":"prQO9Dg8dSYQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Produce the trasformed test images in the destination folder"],"metadata":{"id":"jKlULQGoeAED"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zIG5uWmLyh-c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707759695427,"user_tz":-60,"elapsed":3241,"user":{"displayName":"GAN","userId":"09141957892611562282"}},"outputId":"1a9c1753-a6b4-4dc2-e918-a70ef3a7ea4b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading the trained models from step 200000...\n","Saved real and fake images into /content/drive/MyDrive/Digital/stargan/results/1-images.jpg...\n","Saved real and fake images into /content/drive/MyDrive/Digital/stargan/results/2-images.jpg...\n"]}],"source":["solver.restore_model(200000)\n","\n","data_loader = prova_loader\n","\n","with torch.no_grad():\n","  for i, (x_real, c_org) in enumerate(data_loader):\n","    # Prepare input images and target domain labels.\n","    x_real = x_real.to(solver.device)\n","    c_trg_list = solver.create_labels(c_org, solver.c_dim, solver.selected_attrs)\n","\n","    # Translate images.\n","    x_fake_list = [x_real]\n","    for c_trg in c_trg_list:\n","      x_fake_list.append(solver.G(x_real, c_trg))\n","\n","    # Save the translated images.\n","    x_concat = torch.cat(x_fake_list, dim=3)\n","    result_path = os.path.join(solver.result_dir, '{}-images.jpg'.format(i+1))\n","    save_image(solver.denorm(x_concat.data.cpu()), result_path, nrow=1, padding=0)\n","    print('Saved real and fake images into {}...'.format(result_path))"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":["PZIjmDdlDC1k"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}